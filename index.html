
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-87NG9KQ6C5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-87NG9KQ6C5');
</script>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight:300;
        text-align: center;
    }
    h4 {
        text-align: center;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    hr {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
    
    div {
      text-align: justify;
      text-justify: inter-word;
    }
    .caption {
      font-size: 14px;
      font-style: italic;
      /* text-align: center; */
      text-align: justify;
      text-justify: inter-word;
    }
    .bigvideo {
      /* loop controls muted autoplay */
      width: 1000px;
    }
    .form-control[readonly] {
        background-color: #e9ecef;
        opacity: 1;
    }
    .form-control {
        display: block;
        width: 100%;
        height: calc(9rem + 2px);
        padding: 0.375rem 0.75rem;
        font-size: 0.85rem;
        font-weight: 400;
        line-height: 1.5;
        color: #495057;
        background-color: #fff;
        background-clip: padding-box;
        border: 1px solid #ced4da;
        border-radius: 0.25rem;
        transition: border-color .15s ease-in-out,box-shadow .15s ease-in-out;
        resize: none;
    }
</style>

<html>
  <head>
        <title>HaMeR</title>
        <meta property="og:title" content="HaMeR" />
        <!-- <meta property="og:image" content="https://shubham-goel.github.io/ucmr/resources/images/teaser.png" /> -->
        <link rel="apple-touch-icon" sizes="180x180" href="resources/favicon_io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="resources/favicon_io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="resources/favicon_io/favicon-16x16.png">
        <link rel="manifest" href="resources/favicon_io/site.webmanifest">
        <!-- #TODO -->
        <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=9-Ttb8jsevo" />  -->
  </head>

  <body>
    <!-- <br> -->
    <center>
    <span style="font-size:40px">Reconstructing Hands in 3D with Transformers</span>
    </center>

    <br>
      <table align=center width=1000px>
       <tr>
        <center>
          
          <span style="font-size:20px"><a href="https://geopavlakos.github.io/">Georgios Pavlakos<sup>1</sup> </a></span> &nbsp;&nbsp;&nbsp;

          <span style="font-size:20px"><a href="https://ddshan.github.io/">Dandan Shan<sup>2</sup> </a></span> &nbsp;&nbsp;&nbsp;
          
          <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic<sup>1</sup> </a></span> &nbsp;&nbsp;&nbsp;
              
          <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa<sup>1</sup> </a></span> &nbsp;&nbsp;&nbsp;
          
          <span style="font-size:20px"><a href="https://cs.nyu.edu/~fouhey/">David Fouhey<sup>3</sup> </a></span> &nbsp;&nbsp;&nbsp;
          
          <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik<sup>1</sup> </a></span>
        </center>
     </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><sup>1</sup>University of California, Berkeley</span> &nbsp;
                    <span style="font-size:20px"><sup>2</sup>University of Michigan</span> &nbsp;
                    <span style="font-size:20px"><sup>3</sup>New York University</span>
                </center>
            </td>
        </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">CVPR 2024</span>
                </center>
            </td>
        </tr>
    </table>


    <br>
    <table align=center width=400px>
     <tr>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px">
        <a href="https://arxiv.org/pdf/2312.05251.pdf">
          <!-- <svg style="width:36px;height:36px" viewBox="0 0 24 24">
            <path d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
          </svg> -->
          <img style="width:36px;height:36px" src = "resources/paper.png" alt="Paper"/>
          <br>
          Paper
        </a>
      </span>
       </center>
       </td>


       <!-- <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://arxiv.org/abs/2007.10982">[Arxiv]</a></span>
        </center>
        </td> -->

      <!-- <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://youtu.be/9-Ttb8jsevo">[Video]</a></span>
      </center>
      </td> -->

      <td align=center width=100px>
      <center>
      <span style="font-size:20px">
        <a href="https://github.com/ddshan/hint">
          <!-- <svg style="width:36px;height:36px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M278.9 511.5l-61-17.7c-6.4-1.8-10-8.5-8.2-14.9L346.2 8.7c1.8-6.4 8.5-10 14.9-8.2l61 17.7c6.4 1.8 10 8.5 8.2 14.9L293.8 503.3c-1.9 6.4-8.5 10.1-14.9 8.2zm-114-112.2l43.5-46.4c4.6-4.9 4.3-12.7-.8-17.2L117 256l90.6-79.7c5.1-4.5 5.5-12.3.8-17.2l-43.5-46.4c-4.5-4.8-12.1-5.1-17-.5L3.8 247.2c-5.1 4.7-5.1 12.8 0 17.5l144.1 135.1c4.9 4.6 12.5 4.4 17-.5zm327.2.6l144.1-135.1c5.1-4.7 5.1-12.8 0-17.5L492.1 112.1c-4.8-4.5-12.4-4.3-17 .5L431.6 159c-4.6 4.9-4.3 12.7.8 17.2L523 256l-90.6 79.7c-5.1 4.5-5.5 12.3-.8 17.2l43.5 46.4c4.5 4.9 12.1 5.1 17 .6z"/></svg> -->
          <img style="width:36px;height:36px" src = "resources/big-data.png" alt="Dataset"/>
          <br>
          Data
        </a>
      </span>
      </center>
      </td>

      <td align=center width=100px>
      <center>
      <span style="font-size:20px">
        <a href="https://github.com/geopavlakos/hamer">
          <!-- <svg style="width:36px;height:36px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M278.9 511.5l-61-17.7c-6.4-1.8-10-8.5-8.2-14.9L346.2 8.7c1.8-6.4 8.5-10 14.9-8.2l61 17.7c6.4 1.8 10 8.5 8.2 14.9L293.8 503.3c-1.9 6.4-8.5 10.1-14.9 8.2zm-114-112.2l43.5-46.4c4.6-4.9 4.3-12.7-.8-17.2L117 256l90.6-79.7c5.1-4.5 5.5-12.3.8-17.2l-43.5-46.4c-4.5-4.8-12.1-5.1-17-.5L3.8 247.2c-5.1 4.7-5.1 12.8 0 17.5l144.1 135.1c4.9 4.6 12.5 4.4 17-.5zm327.2.6l144.1-135.1c5.1-4.7 5.1-12.8 0-17.5L492.1 112.1c-4.8-4.5-12.4-4.3-17 .5L431.6 159c-4.6 4.9-4.3 12.7.8 17.2L523 256l-90.6 79.7c-5.1 4.5-5.5 12.3-.8 17.2l43.5 46.4c4.5 4.9 12.1 5.1 17 .6z"/></svg> -->
          <img style="width:36px;height:36px" src = "resources/github-mark.svg" alt="Github code"/>
          <br>
          Code
        </a>
      </span>
      </center>
      </td>

      <td align=center width=100px>
      <center>
      <span style="font-size:20px">
        <a href="https://huggingface.co/spaces/geopavlakos/HaMeR">
          <img style="width:36px;height:36px" src = "resources/huggingface_logo-noborder.svg" alt="Huggging Faces Demo"/>
          <br>
          Demo
        </a>
      </span>
      </center>
      </td>

      <td align=center width=100px>
        <center>
        <span style="font-size:20px">
          <a href="https://colab.research.google.com/drive/1rQbQzegFWGVOm1n1d-S6koOWDo7F2ucu">
            <img style="width:36px;height:36px" src = "resources/Google_Colaboratory_SVG_Logo.svg" alt="Google Colab Demo"/>
            <br>
            Colab
          </a>
        </span>
        </center>
        </td>
  
   </tr>
    </table>

            <!-- <br> -->
            <br>
            <table align=center width=1000px>
                <tr>
                    <td width=1000px>
                      <center>
                          <video loop controls muted autoplay playsinline class="bigvideo">
                            <!-- <source src="https://people.eecs.berkeley.edu/~jathushan/projects/4dhumans/video1.mp4" type="video/mp4"> -->
                            <source src="https://www.cs.utexas.edu/~pavlakos/hamer/resources/video1_results.mp4" type="video/mp4">
                          </video>
                    </center>
                    </td>
                </tr>
                <tr>
                    <td width=200px class="caption">
                          We present HaMeR, an approach for Hand Mesh Recovery from a single image. Given a bounding box of the hand and the hand side (left/right), we use a deep network to reconstruct the hand in 3D in the form of a MANO mesh. Our approach is applied on each frame independently, yet it recovers temporally smooth results. HaMeR is accurate and particularly robust in cases with occlusions, truncations, different skin colors, hands with different appearances (e.g., wearing gloves), as well as hands interacting with other hands or objects.
                    </td>
                </tr>
            </table>

            <br><br>
            <!-- Bold abstract subtitle -->
            <hr>
            <div>
              <h1>Abstract</h1>
              We present an approach that can reconstruct hands in 3D from monocular input. Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based architecture and can analyze hands with significantly increased accuracy and robustness compared to previous work. The key to HaMeR's success lies in scaling up both the data used for training and the capacity of the deep network for hand reconstruction. For training data, we combine multiple datasets that contain 2D or 3D hand annotations. For the deep model, we use a large scale Vision Transformer architecture. Our final model consistently outperforms the previous baselines on popular 3D hand pose benchmarks. To further evaluate the effect of our design in non-controlled settings, we annotate existing in-the-wild datasets with 2D hand keypoint annotations. On this newly collected dataset of annotations, HInt, we demonstrate significant improvements over existing baselines. We will make our code, data and models publicly available upon publication.
            </div>
            <br><br>
            <hr>

            <h1>HaMeR Approach</h1>
            <table>
                <tr>
                  <img class="round" style="width:1000px" src="resources/hamer.png"/>
                </tr>
                <tr>
                  <td class="caption">
                    HaMeR uses a fully transformer-based network design. HaMeR takes as input a single image of a hand and predicts the MANO model parameters, which are used to get the 3D hand mesh.
                  </td>
              </tr>
              </table>
              <br>
              <hr>

            <h1>HInt Dataset</h1>
            <table>
                <tr>
                  <img class="round" style="width:1000px" src="resources/hint.png"/>
                </tr>
                <tr>
                  <td class="caption">
                    We introduce HInt, a dataset focusing on Hand Interactions. We sample frames from New Days of Hands, EpicKitchens-VISOR and Ego4D and we annotate the hands with 2D keypoints. Since the frames are coming from video, they capture more natural interactions of the hands.
                  </td>
              </tr>
              </table>
              <br>
              <hr>

              <h1>Results</h1>
              <h4>Comparisons to existing approaches</h4>
              <table>
                <tr>
                  <td>
                          <video loop controls muted autoplay playsinline class="bigvideo">
                          <source src="https://www.cs.utexas.edu/~pavlakos/hamer/resources/video2_comparison.mp4" type="video/mp4">
                          </video>
                  </td>
                </tr>

                <tr>
                    <td class="caption">
                      We present comparisons to previous state-of-the-art approaches, FrankMocap and Mesh Graphormer for 3D hand mesh reconstruction. HaMeR is more robust and accurate across a variety of hand poses and viewpoints. The baselines suffer from failure cases under occlusions, truncations or unexpected illuminations, while being significantly more jittery than HaMeR.
                    </td>
                </tr>
                </table>
                <br>

                <center><h4>Side-View Visualizations</h4></center>
                <table align=center>
                  <tr>
                    <td>
                      <center>
                                <video loop controls muted autoplay playsinline class="bigvideo"  >
                                <source src="https://www.cs.utexas.edu/~pavlakos/hamer/resources/video3_novelview.mp4" type="video/mp4">
                                </video>

                    </center>
                    </td>
                  </tr>
                  <tr>
                      <td class="caption">
                        In this video, we also visualize the reconstructed hand meshes from a novel viewpoint (specifically a top view, compared to the camera view). The reconstruction from HaMeR are temporally stable even when observed from this novel viewpoint. As in all videos, the results are estimated on a per-frame basis, without any additional smoothing.
                      </td>
                  </tr>
                </table>
                <br>
              <hr>

            <h1>Limitations</h1>
            <table>
                <tr>
                  <td class="center">
                    The main failure modes include spurious hand detections, errors in left/right hand classification, and extremely hard poses. You can see more results on long videos <a href="more.html">here</a>.
                  </td>
              </tr>
             </table>
             <table>
                <tr>
                  <img class="round" style="width:1000px" src="resources/limitation.jpg"/>
                </tr>
                <tr>
                  <td class="caption">
                    Similar to prior work, HaMeR requires the hand side (left/right) information for the input image. When the given hand side is correct (left), the reconstructions align well with the 2D hands; when the given hand side is incorrect (right), the reconstructions are expected to be incorrect (i.e., since the model reconstructs a hand of the opposite side), but HaMeR often returns a reasonable interpretation of the input image.
                  </td>
                </tr>
                </table>
                <table>
                <tr>
                  <img class="round" style="width:1000px" src="resources/failures.jpg"/>
                </tr>
                <tr>
                  <td class="caption">
                    We include representative failure cases of our approach. HaMeR may fail under extreme finger poses, unnatural appearance, extreme occlusions, or unnatural shape (e.g., robotic hand with finger sizes that do not follow the typical human proportions).
                  </td>
              </tr>
              </table>
              <br>

            <table>

              </table>


                <!-- <center><h3>Amodal Completion</h3></center>
                <table align=center width=900px>
                  <tr>
                    <td width=900px>
                      <center>
                          
                                <video loop controls width="900px">
                                <source src="https://people.eecs.berkeley.edu/~jathushan/projects/4dhumans/video4_amodal.mp4" type="video/mp4">
                                </video>

                    </center>
                    </td>
                  </tr>
                  <tr>
                      <td width=600px>
                        <center>
                            <span style="font-size:14px"><i>We can amodally complete our tracklets in frames with missing detections (e.g., occlusions)</i>
                      </center>
                      </td>
                  </tr>
                </table>
                <br> -->
                <hr>

                <table align=center width=1000px>
                  <tr>
                    <td>
                      <div>
                      <h1>Citation</h1>
<textarea id="bibtex" class="form-control" readonly>@inproceedings{pavlakos2024reconstructing,
  title={Reconstructing Hands in 3{D} with Transformers},
  author={Pavlakos, Georgios and Shan, Dandan and Radosavovic, Ilija and Kanazawa, Angjoo and Fouhey, David and Malik, Jitendra},
  booktitle={CVPR},
  year={2024}
}
</textarea>
                    </div>
                  </td>
                </tr>
              </table>
              <br>
              <hr>
                

            <table align=center width=1000px>
              <tr>
                <td>
                  <div>
                    <h1>Acknowledgements</h1>

                    This research was supported by the DARPA Machine Common Sense program, ONR MURI, as well as BAIR/BDD sponsors. We thank members of the BAIR community for helpful discussions. We also thank StabilityAI for supporting us through a compute grant. DF and DS were supported by the National Science Foundation under Grant No. 2006619. This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>. Music credits: <a href="https://vye16.github.io/slahmr/">SLAHMR</a>. Icons: <a href="https://www.flaticon.com/free-icon/paper_2541988" title="paper icons"> Flaticon</a>.
                  </div>
                </td>
              </tr>
            </table>

        <br><br>
</body>
</html>
